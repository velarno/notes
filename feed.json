{
    "version": "https://jsonfeed.org/version/1",
    "title": "Arno&#x27;s Notebook",
    "description": "",
    "home_page_url": "https://velarno.github.io/notes",
    "feed_url": "https://velarno.github.io/notes/feed.json",
    "user_comment": "",
    "author": {
        "name": "Arno Veletanlic"
    },
    "items": [
        {
            "id": "https://velarno.github.io/notes/scraping-the-copernicus-data-store/",
            "url": "https://velarno.github.io/notes/scraping-the-copernicus-data-store/",
            "title": "Scraping the Copernicus Data Store",
            "summary": "I'm currently looking into the Copernicus Data Store website, and after playing a bit with their API, I was looking for a way to automate&hellip;",
            "content_html": "<p>I'm currently looking into <a href=\"https://cds.climate.copernicus.eu/datasets\"  target=\"_blank\"  class=\"extlink extlink-icon-4\" title=\"Copernicus Data Store\" rel=\"noopener\">the Copernicus Data Store website</a>, and after playing a bit with <a href=\"https://cds.climate.copernicus.eu/how-to-api\"  class=\"extlink extlink-icon-4\" title=\"CDS Official API page\" >their API</a>, I was looking for a way to automate away some of my most recurrent data queries.</p>\n<p>But to do anything, you need to have basic information about the datasets CDS provides, and unfortunately their API does not provide a ready-made endpoint to list datasets and their properties/tags.</p>\n<h1>Getting a list of site URLs</h1>\n<h1>HTML approach using `pup`</h1>\n<p><a href=\"https://github.com/ericchiang/pup\"  target=\"_blank\"  class=\"extlink extlink-icon-4\" title=\"Pup Github\" rel=\"noopener\">Pup CLI</a> is a neat tool to quickly parse web content from the command line.</p>\n<p>First, let's fetch the dataset homepage and store it locally:</p>\n<pre class=\"language-bash line-numbers\"><code>curl -sL \"https://cds.climate.copernicus.eu/datasets\" &gt; /tmp/copernicus.home.html</code></pre>\n<p>This file is a mess. I quickly piped it to a LLM (I currently use <a href=\"https://www.cursor.com/\"  target=\"_blank\"  class=\"extlink extlink-icon-4\" title=\"Cursor IDE\" rel=\"noopener noreferrer\">cursor</a>, but a local LLM would do just fine) to figure out the main HTML attributes I should look for to get a <code>href</code> that stats with <code>/datasets/{dataset_id}</code> and sure enough, I have my first pup command:</p>\n<pre class=\"language-bash line-numbers\"><code>pup \\\n'a[tabindex=\"-1\"][aria-hidden=\"true\"][href^=\"/datasets/\"]' \\\n-f /tmp/copernicus.home.html --color</code></pre>\n<p>This does not display enough information for my taste, but at least I have the href, here's a single anchor (the commands outputs as many of these as can be found on the site's page)</p>\n<pre class=\"language-html line-numbers\"><code>&lt;a tabindex=\"-1\" aria-hidden=\"true\" href=\"/datasets/sis-european-wind-storm-reanalysis?tab=overview\"&gt;\n &lt;img src=\"https://cds.climate.copernicus.eu/thumbnails/CwM-xKhoypuPMSljaAxDe0x1KAU=/150x0/filters:format(webp)/object-store.os-api.cci2.ecmwf.int/cci2-prod-catalogue/resources/sis-european-wind-storm-reanalysis/overview_9f914bdde4609de684d1d0c3b71136a26290a31d70fa3772e9980eb9a678cd11.png\" alt=\"\" class=\"w-[150px] h-[150px] hidden md:block object-contain\" loading=\"lazy\"&gt;\n&lt;/a&gt;</code></pre>\n<p>The only interesting thing in here is the anchor's <code>href</code> attribute, so let's modify the command to only get these:</p>\n<pre class=\"language-bash line-numbers\"><code>pup \\\n'a[tabindex=\"-1\"][aria-hidden=\"true\"][href^=\"/datasets/\"] attr{href}' \\\n-f /tmp/copernicus.home.html \\\n--color</code></pre>\n<p>and what we get is a beautiful list of links !</p>\n<pre class=\"language-bash line-numbers\"><code>/datasets/satellite-soil-moisture?tab=overview\n/datasets/reanalysis-era5-single-levels?tab=overview\n/datasets/derived-era5-single-levels-daily-statistics?tab=overview\n/datasets/derived-utci-historical?tab=overview\n/datasets/reanalysis-era5-land-timeseries?tab=overview\n# ... list goes on</code></pre>\n<h2>How many links did we fetch ?</h2>\n<p>Our goal is to fetch all the datasets currently published by the Copernicus CDS, so let's check we have a number close to 123, increasing the <code>limit</code> might be helpful</p>\n<pre class=\"language-bash line-numbers\"><code>curl -sL 'https://cds.climate.copernicus.eu/datasets?q=&amp;limit=150' | \\\npup 'article a[href$=\"?tab=overview\"][href^=\"/datasets/\"]:not([aria-hidden]) attr{href}' | \\\nwc -l</code></pre>\n<p>This sadly only returnsÂ <code>30</code>, most likely because the CDS search URL has a lazy-loading feature which only loads the first 30 datasets.</p>\n<h1>Sitemap approach with `yq`</h1>\n<p>By checking various info about the site, I noticed theÂ <code>/sitemap.xml</code> endpoint had a lot of information aboutÂ <code>/datasets/</code> pages, so let's try using this instead, we'll need <a href=\"https://mikefarah.gitbook.io/yq\"  target=\"_blank\"  class=\"extlink extlink-icon-4\" title=\"yq documentation\" rel=\"noopener noreferrer\">the yq CLI</a>.</p>\n<pre class=\"language-bash line-numbers\"><code>curl -sL 'https://cds.climate.copernicus.eu/sitemap.xml' \\\n&gt; /tmp/coperniucs.sitemap.xml \\\n&amp;&amp; yq \\\n'.urlset.url[] | select(.loc | contains(\"/datasets/\")) | .loc' \\\n/tmp/coperniucs.sitemap.xml -o=json \\\n| wc -l</code></pre>\n<p>This unfortunately returns onlyÂ <code>50</code> - let's try the most powerful approach next.</p>\n<h1>Using playwright with `uv`</h1>\n<p>The next best thing to a bash script is a script you can easily run without installing multiple dependencies. After a bit of back-and-forth, I ended up with the <a href=\"https://gist.github.com/velarno/fba2ade917502c4ca795a0e88712b812\"  target=\"_blank\"  class=\"extlink extlink-icon-4\" title=\"gist python script playwright\" rel=\"noopener noreferrer\">following python script</a>. Noteworthy bits are:</p>\n<ol>\n<li>Few dependencies, allowing easy one-liners withÂ <code>uv run</code></li>\n<li>Uses <code>page.evaluate(\"window.scrollY\")</code> in a loop to scroll to the bottom of the page (idea is from <a href=\"https://www.xiegerts.com/post/infinite-scroll-scrapy-playwright/\"  target=\"_blank\"  class=\"extlink extlink-icon-4\" title=\"xiegerts post infinite scroll scraping\" rel=\"noopener\">this neat blogpost</a>)</li>\n<li>Parses everyÂ <code>&lt;article&gt;</code> element and nested elements to get dataset information</li>\n<li>Outputs everything as a clean JSON object</li>\n</ol>\n<p>If you want a quick summary of these key steps, I'll cover these below.</p>\n<h2>Dependencies</h2>\n<p>This script only requiresÂ <code>playwright</code>, with the caveat that playwright needs to be initialized by runningÂ <code>playwright install</code> in the terminal. We use a subprocess for this, it's not ideal but it works.</p>\n<pre class=\"language-python line-numbers\"><code>import asyncio\nimport subprocess\nfrom playwright.async_api import async_playwright\n\nsubprocess.run([\"playwright\", \"install\"], check=True) # install playwright\n\nURL = \"https://cds.climate.copernicus.eu/datasets\"</code></pre>\n<h2>Infinite Scroll scraping</h2>\n<p>In order to avoid the infinite scroll issue we stumbled into before, we scroll in a loop until scrolling isn't possible (we assume the timeout is enough to load the next batch of data, adjust it if needed)</p>\n<pre class=\"language-python line-numbers\"><code># Scroll using window.scrollBy and window.scrollY until the bottom\nlast_position = await page.evaluate(\"window.scrollY\")\nwhile True:\n   await page.evaluate(\"window.scrollBy(0, 700)\")\n   await page.wait_for_timeout(750)\n   current_position = await page.evaluate(\"window.scrollY\")\n   if current_position == last_position:\n        break\n   last_position = current_position</code></pre>\n<p>This is a bit slow (multiple 750ms waits), so ideally you would add logging or progress update output, but here we're just prototyping.</p>\n<h2>Parsing the data</h2>\n<p>Getting all the relevant dataset information involves pretty boring HTML parsing, but we see similarities with theÂ <code>pup</code> commands we used above:</p>\n<pre class=\"language-python line-numbers\"><code>link_el = await article.query_selector(\"a[href*='/datasets/']\")</code></pre>\n<p>this fetches the same anchors as the pup scripts. One thing that would be a bit difficult to do withÂ <code>pup</code> is perhaps the mix of very different queries to get all the relevant information. One vaguely interesting aspect is fetching dataset \"tags\" which are displayed asÂ <code>&lt;button&gt;</code> and <code>input</code> elements:</p>\n<pre class=\"language-python line-numbers\"><code># Tags (checkboxes/buttons)\ntag_els = await article.query_selector_all(\n    (\n        \"button, [role='checkbox'],\" +\n        \" .cds-portal-tag,\" +\n        \" .cds-portal-chip,\" +\n        \" input[type='checkbox'] + label\"\n    )\n)\ntags = []\nfor tag_el in tag_els:\n    tag_text = await tag_el.inner_text()\n    if tag_text:\n        tags.append(tag_text.strip())</code></pre>\n<h2>The one-liner script</h2>\n<p>If you want to test this script, it's quite simple to run it withÂ <code>uv run</code> since it supports remote scripts:</p>\n<pre class=\"language-python line-numbers\"><code>uv run --with playwright \\\nhttps://gist.githubusercontent.com/velarno/fba2ade917502c4ca795a0e88712b812/raw/88f816389c13b53bc402ed9edc3f41db87c33443/fetch_cds_datasets.py</code></pre>\n<p>Here's the first few lines of the output, confirming we can fetch all datasets:</p>\n<pre class=\"language-bash line-numbers\"><code>Installed 4 packages in 33ms\nFound 123 datasets\n{'link': '/datasets/derived-utci-historical?tab=overview', 'title': 'Thermal comfort indices derived from ERA5 reanalysis', 'description': 'This dataset provides a complete historical reconstruction for a set of indices representing human thermal stress and discomfort in outdoor conditions. This dataset, also known as ERA5-HEAT (Human thErmAl comforT) represents the current state-of-the-art for bioclimatology data record production. The...', 'tags': ['Reanalysis', 'Copernicus C3S', 'Global', 'Past', 'Atmosphere (surface)', 'Atmosphere (upper air)']}</code></pre>\n<p>Hooray ! ðŸ¥³ That's all for today, in the next few days I'll probably clean this up and keep working on CDS related datasets. I might post one or two notes if I come across anything noteworthy.</p>",
            "author": {
                "name": "Arno Veletanlic"
            },
            "tags": [
                   "yq",
                   "web",
                   "uv",
                   "scraping",
                   "python",
                   "pup",
                   "html"
            ],
            "date_published": "2025-07-01T23:25:09+02:00",
            "date_modified": "2025-07-01T23:31:47+02:00"
        }
    ]
}
